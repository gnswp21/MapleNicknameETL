FROM openjdk:11
# install spark


# step 1: install spark
COPY dockerbuild/local/spark-3.1.2-bin-hadoop3.2.tgz  /usr/lib/spark-3.1.2-bin-hadoop3.2.tgz
WORKDIR /usr/lib
RUN tar xvf spark-3.1.2-bin-hadoop3.2.tgz  && rm *.tgz
RUN ln -s spark-3.1.2-bin-hadoop3.2 spark

## step 2: Add required JARs
ARG JAR_HOME=/usr/lib/spark/jars/
ADD https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar $JAR_HOME
ADD https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.2/kafka-clients-2.6.2.jar $JAR_HOME
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar $JAR_HOME
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar $JAR_HOME
# Spark version related.
ADD https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar $JAR_HOME
ADD https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar $JAR_HOME
# Update permissions
RUN chmod -R +r /usr/lib/spark/jars



# step 3: install third party python packages
## Install Python zip
RUN apt-get update && \
    apt-get install -y python3 python3-pip && \
    apt-get install -y zip
## third party package for model
RUN pip3 install --upgrade pip
RUN pip3 install 'transformers[torch]'
RUN pip3 install pandas
RUN pip3 install scikit-learn

## pyspark
RUN pip3 install pyspark

# step 4: copy over custom ETL python modules
COPY src /etl/src
RUN cd /etl \
    && zip -r dependency_packages.zip src/ \
    && chmod +x /etl/dependency_packages.zip \
    && cd -
COPY data /etl/data
# step 5: copy Java/Spark/Hadoop related config

# set configurations, ENV variables
ENV SPARK_HOME=/usr/lib/spark
ENV PATH=$PATH:$SPARK_HOME/bin

COPY dockerbuild/local/spark-defaults.conf $SPARK_HOME/conf/spark-defaults.conf
RUN mkdir $SPARK_HOME/logs
WORKDIR /etl


